{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Neural Network Features\n",
    "\n",
    "## 1\n",
    "We've spent a good deal of energy looking at the patterns of weights learned by classifiers such as multilayer perceptrons and such.  This is very interesting, however it doesn't provide a means of interrogating what's happening beyond the very first layer (where the weights are amenable to reshaping into an image).  This is especially acute as neural networks become *deep* and have more exotic architectures.  Here, we will interrogate the \"nodes\" (abstractly in this case) of an enormous convolutional neural network, to figure out what they are looking for.  In so doing, we will see that deep neural networks represent and recognize objects by constructing a hierarchy of visual concepts that are sequentially combined to form higher-level representations.   \n",
    "\n",
    "## 2 VGG\n",
    "For real industrial scale applications, many interesting CNNs already exist and are available *pre-trained*, and we can query their behavior.  One notable example is the neural network VGG16, which has the following architecture:\n",
    "<img src=\"vgg.png\" />\n",
    "This is a so-called *deep network* because it has many layers.  It is also interesting because it doesn't do much fancy stuff (take advanced ML or Computer Vision if you want to know about fancy stuff), and the features that it extracts end up being relatively comprehensible.  Note that VGG was trained on (a subset of) the [ImageNet](http://www.image-net.org/) dataset, which contains 15M labelled images and more than 20000 classes.  VGG was trained on more than a million of these and can recognize 1k different classes.  It's very large: 138M parameters and was trained for three weeks on an array of four Titan XP GPUs.  \n",
    "\n",
    "We have straightforward access to it via pytorch, specifically the torchvision package.  It might take you a bit of time to download the weights the first time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as tvm\n",
    "model = tvm.vgg16(pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have VGG, we'd like to create a mechanism that determines what its nodes are looking for.  Stated more precisely, one way to solve this problem is to explicitly construct an image that maximizes a given node's activation: what picture makes that node light up?  The following class solves that problem, by computing gradients of a specified nodal activation with respect to an (initially random) input image, and adjusting that image so as the activation is maximized.  There are also a few tricks invoked to make these images more compelling: a bit of regularization, some upscaling and restarting from upscaled images, etc.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "class FeatureVisualizer(object):\n",
    "    \"\"\"Class for visualizing features in the pretrained pytorch VGG model\"\"\"\n",
    "    def __init__(self,model):\n",
    "        \"\"\"Pass in the correct model\"\"\"\n",
    "        self.model = model  \n",
    "           \n",
    "    def create_feature_maximizer(self,layer_index,kernel_index,n_steps=10,gaussian_blur=0.1,imsize=64,upscaling_steps=10,upscaling_factor=1.2):\n",
    "        \"\"\"Finds an image that maximizes the activations induced by a given kernel\n",
    "        Arguments:\n",
    "           layer_index: the index of the layer in question\n",
    "           kernel_index: the index of the kernel in that layer\n",
    "           n_steps: number of optimization steps to take\n",
    "           gaussian_blur: how much to blur the resulting image to minimize high frequency noise\n",
    "           imsize: how big an image to make\n",
    "        Outputs: \n",
    "           img: the image that maximizes the given activation\n",
    "        \"\"\"\n",
    "        \n",
    "        # Instantiate a random image of the appropriate size and convert it to a variable\n",
    "        img = torch.from_numpy(np.uint8(np.random.uniform(-125, 125, (3,imsize, imsize)))/255)\n",
    "        img = img.to(torch.float32).detach().unsqueeze(0)\n",
    "        img = img.clone().detach().requires_grad_(True)\n",
    "\n",
    "\n",
    "        for j in range(upscaling_steps):\n",
    "            \n",
    "            sz = int(upscaling_factor**j * imsize)  # calculate new image size\n",
    "            img = resize(img.detach().numpy().squeeze().transpose(1,2,0), (sz, sz)).transpose(2,0,1)  # scale image up\n",
    "            img = torch.from_numpy(img)\n",
    "            img = img.to(torch.float32).detach().unsqueeze(0)\n",
    "            img = img.clone().detach().requires_grad_(True)    \n",
    "            print(img.shape)\n",
    "            \n",
    "            # Instantiate an optimizer, with the pixel values of the input image as the variable to be optimized\n",
    "            optimizer = torch.optim.Adam([img], lr=1e-1,weight_decay=1e-6)\n",
    "            \n",
    "            # Optimize\n",
    "            for i in range(n_steps):\n",
    "                # Zero the gradient buffer\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "                # Create a data structure to store intermediate network outputs\n",
    "                outputs = []\n",
    "                def hook_fn(module, input, output):\n",
    "                    outputs.append(output)\n",
    "            \n",
    "                # Create a function that will store a given layer's output\n",
    "                hook = model.features[layer_index].register_forward_hook(hook_fn)\n",
    "            \n",
    "                # Run the model on the current image value\n",
    "                output = model(img)\n",
    "            \n",
    "                # Define the loss using the layer output\n",
    "                loss = -outputs[0][0,kernel_index].mean()\n",
    "            \n",
    "                # Print some stats\n",
    "                print(\"Negative Mean activation for layer:\",layer_index,\", kernel index\", kernel_index,\": \",loss.item())\n",
    "            \n",
    "                # Compute the gradient\n",
    "                loss.backward()\n",
    "            \n",
    "                # Update the pixel values\n",
    "                optimizer.step()\n",
    "            \n",
    "                # Destroy the intermediate output function\n",
    "                hook.remove()\n",
    "            \n",
    "        # Convert img to w x h x channel, convert to numpy, remove batch dimension\n",
    "        img = np.moveaxis(img.detach().numpy().squeeze(),0,2)\n",
    "        \n",
    "        img_new = img - img.mean(axis=(0,1))# + np.array([0.485, 0.456, 0.406])\n",
    "        img_new /= img_new.std(axis=(0,1))\n",
    "        img_new *= np.array([0.229, 0.224, 0.225])\n",
    "        img_new += np.array([0.485, 0.456, 0.406])\n",
    "              \n",
    "        return img_new\n",
    "    \n",
    "    def get_mean_layer_activations(self,image_path,layer_index,imsize=224):\n",
    "        \"\"\"Computes the mean activations of a layer for a given image\n",
    "        Arguments:\n",
    "          image_path: the path to the image we want to run the network on\n",
    "          layer_index: the layer that we want to compute the mean activations for\n",
    "        Outputs:\n",
    "          mean_layer_activations: the mean of each feature map in a layer after being evaluated on an image\n",
    "        \"\"\"\n",
    "        # Load and normalize image to format expected by VGG network\n",
    "        loader = transforms.Compose([transforms.Resize(imsize),transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])])\n",
    "        \n",
    "        # Define data structure to hold intermediate network output\n",
    "        outputs= []\n",
    "        def hook_fn(module, input, output):\n",
    "            outputs.append(output)\n",
    "        \n",
    "        # Define a function to load the image\n",
    "        def image_loader(image_name):\n",
    "            \"\"\"load image, returns cuda tensor\"\"\"\n",
    "            image = Image.open(image_name)\n",
    "            image = loader(image).float()\n",
    "            image = Variable(image, requires_grad=True)\n",
    "            image = image.unsqueeze(0)\n",
    "            return image\n",
    "\n",
    "        # Load the image\n",
    "        img = image_loader(image_path)\n",
    "\n",
    "        # Run the model and save intermediate output\n",
    "        hook = self.model.features[layer_index].register_forward_hook(hook_fn)\n",
    "        output = self.model(img)\n",
    "        hook.remove()\n",
    "        mean_layer_activations = np.mean(outputs[0].detach().cpu().numpy().squeeze(),axis=(1,2))\n",
    "        return mean_layer_activations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to determine what layer to query.  We can do this by looking at the model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what type of image the first kernel in the first convolutional layer (after applying relu) is looking for.  This is Layer 1, kernel 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (10,10)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "layer = 1\n",
    "kernel = 0\n",
    "fv = FeatureVisualizer(model)\n",
    "img = fv.create_feature_maximizer(layer,kernel,n_steps=20,imsize=32,upscaling_steps=10,upscaling_factor=1.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining features at different layers\n",
    "Produce images like the ones above for several other kernels in this CNN layer.  What type of features does this kernel seem to be focusing on.  Do the same for different layers.  What can you say about the complexity of the features being detected at deeper levels of the neural network? (you will find that this procedure is a bit unstable and may not work for layers very late in the network, especially the last one).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Targeted extraction\n",
    "We can also examine what the network is focusing when extracting features *for a specific image*.  Download your own image of a common object (a cat, or a truck, or something simple), and use the *get_mean_layer_activations* method in the class given above to get the mean activations for a given layer as a function of the kernel index.  Identify which kernel for a given layer is most strongly activated by your particular image.  Then, generate the image that maximizes the activation for that layer and kernel.  What feature of your chosen image is the network looking at?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e.g.\n",
    "mean_layer_activations = fv.get_mean_layer_activations('cat.jpg',15)\n",
    "plt.plot(mean_layer_activations)\n",
    "print(np.argmax(mean_layer_activations))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
