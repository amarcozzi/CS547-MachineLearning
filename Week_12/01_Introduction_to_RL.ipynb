{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Watch all videos and read the review sections.\n",
    "It's a lot of material, but we need to get through it before we can do some coding exercises.\n",
    "\n",
    "# 1.0 Introduction to RL\n",
    "---\n",
    "\n",
    "[Intro](https://www.youtube.com/watch?v=6jSFl5kxIBs&t=2s)\n",
    "\n",
    "[The Setting](https://www.youtube.com/watch?v=nh8Gwdu19nc&t=21s)\n",
    "\n",
    "[The Setting Revisited](https://www.youtube.com/watch?v=V6Q1uF8a6kA&t=8s)\n",
    "\n",
    "[Episodic vs. Continuing Tasks](https://www.youtube.com/watch?v=E1I-BPanSM8&t=51s)\n",
    "\n",
    "[Reward Hypothesis](https://www.youtube.com/watch?v=uAqNwgZ49JE&t=10s)\n",
    "\n",
    "[Goals and Rewards: 1](https://www.youtube.com/watch?v=XPnj3Ya3EuM&t=1s)\n",
    "\n",
    "[Goals and Rewards: 2](https://www.youtube.com/watch?v=pVIFc72VYH8&t=4s)\n",
    "\n",
    "[Cumulative Rewards](https://www.youtube.com/watch?v=ysriH65lV9o&t=62s)\n",
    "\n",
    "[Discounted Rewards](https://www.youtube.com/watch?v=opXGNPwwn7g&t=58s)\n",
    "\n",
    "[MDPs: 1](https://www.youtube.com/watch?v=NBWbluSbxPg&t=1s)\n",
    "\n",
    "[MDP: 2](https://www.youtube.com/watch?v=CUTtQvxKkNw&t=3s)\n",
    "\n",
    "[MDP: 3](https://www.youtube.com/watch?v=UlXHFbla3QI&t=17s)\n",
    "\n",
    "## Review\n",
    "---\n",
    "\n",
    "### The Setting, Revisited\n",
    "- The reinforcement learning (RL) framework is characterized by an agent learning to interact with its environment.\n",
    "- At each time step, the agent receives the environment's state (the environment presents a situation to the agent), and the agent must choose an appropriate action in response. One time step later, the agent receives a reward (the environment indicates whether the agent has responded appropriately to the state) and a new state.\n",
    "- All agents have the goal to maximize expected cumulative reward, or the expected sum of rewards attained over all time steps.\n",
    "\n",
    "### Episodic vs. Continuing Tasks\n",
    "- A task is an instance of the reinforcement learning (RL) problem.\n",
    "- Continuing tasks are tasks that continue forever, without end.\n",
    "- Episodic tasks are tasks with a well-defined starting and ending point.\n",
    "- In this case, we refer to a complete sequence of interaction, from start to finish, as an episode.\n",
    "- Episodic tasks come to an end whenever the agent reaches a terminal state.\n",
    "\n",
    "### The Reward Hypothesis\n",
    "- Reward Hypothesis: All goals can be framed as the maximization of (expected) cumulative reward.\n",
    "\n",
    "### Cumulative Reward\n",
    "- The return at time step $t$ is:\n",
    "$$G_t := R_{t+1} + R_{t+2} + R_{t+3} + \\ldots$$\n",
    "\n",
    "- The agent selects actions with the goal of maximizing expected (discounted) return. (Note: discounting is covered in the next concept.)\n",
    "\n",
    "### Discounted Return\n",
    "- The discounted return at time step t is:\n",
    "$$G_t := R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots$$\n",
    "\n",
    "- The discount rate $\\gamma$ is something that you set, to refine the goal that you have the agent.\n",
    "    - It must satisfy $0 \\leq \\gamma \\leq 1$.\n",
    "    - If $\\gamma=0$, the agent only cares about the most immediate reward.\n",
    "    - If $\\gamma=1$, the return is not discounted.\n",
    "    - For larger values of $\\gamma$, the agent cares more about the distant future. Smaller values of $\\gamma$ result in more extreme discounting, where - in the most extreme case - agent only cares about the most immediate reward.\n",
    "\n",
    "### MDPs and One-Step Dynamics\n",
    "- The state space $\\mathcal{S}$ is the set of all (nonterminal) states.\n",
    "- In episodic tasks, we use $\\mathcal{S}^+$ to refer to the set of all states, including terminal states.\n",
    "- The action space $\\mathcal{A}$ is the set of possible actions. (Alternatively, $\\mathcal{A}(s)$ refers to the set of possible actions available in state $s \\in \\mathcal{S}$.)\n",
    "- The one-step dynamics of the environment determine how the environment decides the state and reward at every time step. This is often called the transition function:\n",
    "$$p(s',r|s,a) \\doteq \\mathbb{P}(S_{t+1}=s', R_{t+1}=r|S_{t} = s, A_{t}=a) \\text{ for each possible } s', r, s, \\text{and } a$$\n",
    "\n",
    "- A (finite) Markov Decision Process (MDP) is defined by:\n",
    "    - a (finite) set of states $\\mathcal{S}$ (or $\\mathcal{S}^+$, in the case of an episodic task)\n",
    "    - a (finite) set of actions $\\mathcal{A}$\n",
    "    - a set of rewards $\\mathcal{R}$\n",
    "    - the one-step dynamics of the environment\n",
    "    - the discount rate $\\gamma \\in [0,1]$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.0 Monte Carlo Methods\n",
    "---\n",
    "\n",
    "[Review](https://www.youtube.com/watch?v=3H5x0lstvmo)\n",
    "\n",
    "[Grid World Example](https://www.youtube.com/watch?v=Lwibg_IfmrA)\n",
    "\n",
    "[Monte Carlo Methods](https://www.youtube.com/watch?v=titaMCRl224)\n",
    "\n",
    "[MC Prediction Part: 1](https://www.youtube.com/watch?v=6ts9gdIS6vg)\n",
    "\n",
    "[MC Prediction Part: 2](https://www.youtube.com/watch?v=jR49ZyKuJ98)\n",
    "\n",
    "[MC Prediction Part: 3](https://www.youtube.com/watch?v=9LP6uXdmWxQ)\n",
    "\n",
    "[Greedy Policy](https://www.youtube.com/watch?v=DH6c-aODMLU)\n",
    "\n",
    "[Epsilon Greedy Policy](https://www.youtube.com/watch?v=PxJMtlR06MY)\n",
    "\n",
    "[Incremental Mean](https://www.youtube.com/watch?v=h-8MB7V1LiE)\n",
    "\n",
    "[Constant Alpha](https://www.youtube.com/watch?v=QFV1nI9Zpoo)\n",
    "\n",
    "## Review\n",
    "---\n",
    "\n",
    "### Monte Carlo Methods\n",
    "- Monte Carlo methods - even though the underlying problem involves a great degree of randomness, we can infer useful information that we can trust just by collecting a lot of samples.\n",
    "- The **equiprobable random policy** is the stochastic policy where - from each state - the agent randomly selects from the set of available actions, and each action is selected with equal probability.\n",
    "\n",
    "### MC Prediction\n",
    "- Algorithms that solve the prediction problem determine the value function $v_{\\pi}$ (or $q_{pi}$) corresponding to a policy $\\pi$.\n",
    "- When working with finite MDPs, we can estimate the action-value function $q_{\\pi}$ corresponding to a policy $\\pi$ in a table known as a Q-table. This table has one row for each state and one column for each action. The entry in the s-th row and a-th column contains the agent's estimate for expected return that is likely to follow, if the agent starts in state s, selects action a, and then henceforth follows the policy $\\pi$.\n",
    "- Each occurrence of the state-action pair $s,a (s\\in\\mathcal{S},a\\in\\mathcal{A})$ in an episode is called a visit to s,a.\n",
    "- There are two types of MC prediction methods (for estimating $q_{\\pi}$\n",
    "    - First-visit MC estimates $q_{\\pi}(s,a)$ as the average of the returns following only first visits to s,a (that is, it ignores returns that are associated to later visits).\n",
    "    - Every-visit MC estimates $q_{\\pi}(s,a)$ as the average of the returns following all visits to s,a.\n",
    "\n",
    "### Greedy Policies\n",
    "- A policy is greedy with respect to an action-value function estimate Q if for every state $s\\in\\mathcal{S}$, it is guaranteed to select an action $a\\in\\mathcal{A}(s)$ such that $a = \\arg\\max_{a\\in\\mathcal{A}(s)}Q(s,a)$. (It is common to refer to the selected action as the greedy action.)\n",
    "- In the case of a finite MDP, the action-value function estimate is represented in a Q-table. Then, to get the greedy action(s), for each row in the table, we need only select the action (or actions) corresponding to the column(s) that maximize the row.\n",
    "\n",
    "### Epsilon-Greedy Policies\n",
    "- A policy is $\\epsilon$ greedy with respect to an action-value function estimate Q if for every state $s\\in\\mathcal{S}$\n",
    "    - with probability $1-\\epsilon$, the agent selects the greedy action, and\n",
    "    - with probability $\\epsilon$, the agent selects an action uniformly at random from the set of available (non-greedy AND greedy) actions.\n",
    "\n",
    "### MC Control\n",
    "- Algorithms designed to solve the control problem determine the optimal policy $\\pi$ from interaction with the environment.\n",
    "- The Monte Carlo control method uses alternating rounds of policy evaluation and improvement to recover the optimal policy.\n",
    "\n",
    "### Exploration vs. Exploitation\n",
    "- All reinforcement learning agents face the Exploration-Exploitation Dilemma, where they must find a way to balance the drive to behave optimally based on their current knowledge (exploitation) and the need to acquire knowledge to attain better judgment (exploration).\n",
    "- In order for MC control to converge to the optimal policy, the Greedy in the Limit with Infinite Exploration (GLIE) conditions must be met:\n",
    "every state-action pair s, as,a (for all $s\\in\\mathcal{S}$ and $a\\in\\mathcal{A}(s)$) is visited infinitely many times, and\n",
    "the policy converges to a policy that is greedy with respect to the action-value function estimate Q.\n",
    "\n",
    "### Incremental Mean\n",
    "- (In this concept, we amended the policy evaluation step to update the Q-table after every episode of interaction.)\n",
    "\n",
    "### Constant-alpha\n",
    "- (In this concept, we derived the algorithm for constant-$\\alpha$ MC control, which uses a constant step-size parameter $\\alpha$.)\n",
    "- The step-size parameter $\\alpha$ must satisfy $0 < \\alpha \\leq 10$. Higher values of $\\alpha$ will result in faster learning, but values of $\\alpha$ that are too high can prevent MC control from converging to $\\pi_*$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
