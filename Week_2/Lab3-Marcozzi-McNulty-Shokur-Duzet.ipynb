{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lab 3 Group:\n",
    "Anthony Marcozzi\n",
    "Courtney Duzet\n",
    "Sean McNulty\n",
    "Ed Shokur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In class exercise: Gene Sequence Clustering\n",
    "\n",
    "### Training a Markov model\n",
    "Load the file genes\\_training.p, which is available in this homework archive.  genes\\_training.p contains 2000 sequences, with each sequence $\\mathbf{s}$ consisting of 20 nucleobases $s_i \\in \\mathrm{Nu},\\; \\mathrm{Nu} = \\{A,T,G,C\\}$.  Each of these sequences is generated from one of two separate Markov processes.  The label (aka class) of the process that generated the sequence is given in the dataset.\n",
    "\n",
    "Learn the Markov model for each class given the training data.  **To do this, for each class compute the prior probability $\\mathbf{\\pi}_c$ of each nucleobase (the relative frequency of each nucleobase for each class, a vector of length 4) and the matrix of transition probabilities**\n",
    "$$\n",
    "\\mathcal{A}_{c,kj} = P(s_{i+1}=\\mathrm{Nu}_j|s_{i}=\\mathrm{Nu}_k),\n",
    "$$\n",
    "which is a 4 by 4 matrix.  As a quick sanity check, each row of $\\mathcal{A}_c$ should sum to one.  **Using these priors and transition matrices, write a function that generates a new sequence given the class**, i.e. simulates a data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['G',\n",
       " 'A',\n",
       " 'G',\n",
       " 'A',\n",
       " 'C',\n",
       " 'A',\n",
       " 'A',\n",
       " 'C',\n",
       " 'A',\n",
       " 'C',\n",
       " 'A',\n",
       " 'G',\n",
       " 'T',\n",
       " 'A',\n",
       " 'C',\n",
       " 'A',\n",
       " 'G',\n",
       " 'A',\n",
       " 'C',\n",
       " 'C']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Define some useful constants\n",
    "N_nucleobases = 4\n",
    "N_classes = 2\n",
    "nucleobases = ['A','T','G','C']\n",
    "\n",
    "# Load the training data using pickle\n",
    "sequences,labels = pickle.load(open('04-Markov-Models-master/genes_training.p','rb'))\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Initialize the class priors and transition matrices\n",
    "A_0 = np.zeros((N_nucleobases,N_nucleobases))\n",
    "A_1 = np.zeros((N_nucleobases,N_nucleobases))\n",
    "\n",
    "##### Train prior #####\n",
    "\n",
    "#! Compute class priors\n",
    "pi_0 = labels[labels==0].size / labels.size\n",
    "pi_1 = 1 - pi_0\n",
    "\n",
    "#! Compute unconditional nucleobase probabilities\n",
    "nucleobase_priors = {'A': 0, 'T': 0, 'C': 0, 'G': 0}\n",
    "num_chars = 0\n",
    "for seq in sequences:\n",
    "    for char in seq:\n",
    "        nucleobase_priors[char] += 1\n",
    "        num_chars += 1\n",
    "\n",
    "# Convert the nucleobase count to a probability\n",
    "nucleobase_priors['A'] /= num_chars\n",
    "nucleobase_priors['T'] /= num_chars\n",
    "nucleobase_priors['C'] /= num_chars\n",
    "nucleobase_priors['G'] /= num_chars\n",
    "\n",
    "# Compute conditional nucelobase probabilities\n",
    "pi_0 = {'A': 0, 'T': 0, 'C': 0, 'G': 0}\n",
    "pi_1 = {'A': 0, 'T': 0, 'C': 0, 'G': 0}\n",
    "num_0 = 0\n",
    "num_1 = 0\n",
    "for i in range(len(sequences)):\n",
    "    seq = sequences[i]\n",
    "    label = labels[i]\n",
    "    for char in seq:\n",
    "        if label == 0:\n",
    "            pi_0[char] += 1\n",
    "            num_0 += 1\n",
    "        else:\n",
    "            pi_1[char] += 1\n",
    "            num_1 += 1\n",
    "            \n",
    "# Convert the nucleobase count to a probability\n",
    "pi_0['A'] /= num_0\n",
    "pi_0['T'] /= num_0\n",
    "pi_0['C'] /= num_0\n",
    "pi_0['G'] /= num_0\n",
    "pi_1['A'] /= num_1\n",
    "pi_1['T'] /= num_1\n",
    "pi_1['C'] /= num_1\n",
    "pi_1['G'] /= num_1\n",
    "\n",
    "##### Train transition matrix #####\n",
    "nbd = {'A': 0, 'T': 1, 'C': 2, 'G': 3}\n",
    "for s,l in zip(sequences,labels):\n",
    "    sequence_length = len(s)\n",
    "    for p in range(sequence_length-1):\n",
    "        #! s is a length 20 sequence of nucleoboases, for all s, count the number of times that a nucleobase\n",
    "        #! transitions to another nucleobase and record this information in the appropriate transition matrix (A_0 or A_1)\n",
    "        n1 = nbd[s[p]]\n",
    "        n2 = nbd[s[p+1]]\n",
    "        if l == 0:\n",
    "            A_0[n1, n2] += 1\n",
    "        else:\n",
    "            A_1[n1, n2] += 1\n",
    "\n",
    "# Convert from counts to probabilities by row normalization\n",
    "A_0/=A_0.sum(axis=1)[:,np.newaxis]\n",
    "A_1/=A_1.sum(axis=1)[:,np.newaxis]\n",
    "\n",
    "##### Generate a synthetic sequence #####\n",
    "def generate_new_sequence(A,pi,n=20):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A -> Nucleobase transition matrix\n",
    "    pi -> Prior\n",
    "    n -> length of sequence to generate\n",
    "    \"\"\"\n",
    "    # Draw from the prior for the first nucleobase\n",
    "    s = [np.random.choice(nucleobases,p=pi)]\n",
    "    #! Write the code that uses the transition matrix to produce a length n sample\n",
    "    for i in range(1, n):\n",
    "        index = nbd[s[i-1]]\n",
    "        new = np.random.choice(nucleobases, p=A[index, :])\n",
    "        s.append(new)\n",
    "    return s\n",
    "\n",
    "priors = [nucleobase_priors['A'], nucleobase_priors['T'], nucleobase_priors['C'], nucleobase_priors['G']]\n",
    "generate_new_sequence(A_1, priors, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Markov classifier\n",
    "Having the prior and transition probabilities gives you the ability to evaluate the likelihood of a sequence for a given class as:\n",
    "$$\n",
    "P(\\mathbf{s}) = P(s_1|\\mathbf{\\pi}_c) \\prod_{i=1}^{n-1} P(s_{i+1}|s_{i},\\mathcal{A}_c),\n",
    "$$\n",
    "where $\\mathbf{\\pi}_c$ is the class-conditioned prior probability, and $\\mathcal{A}_c$ is the class-conditioned transition matrix.  Comparing the computed likelihood for a given sequence between different classes forms the basis of a classifier in a very similar manner to naive Bayes.  The difference this time is that now each random variable depends on the one before it in the sequence, whereas in naive Bayes we assumed that all the random variables were independent.\n",
    "\n",
    "Load the file genes\\_test.p, which is similar to genes\\_training.p.  **For each sequence, compute the likelihood for both classes and assign a label.  Compare this predicted label to the known one, and report the test set accuracy**.  As a point of comparison, my implementation achieved 98.7\\% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.72745986e-02 9.62725401e-01]\n",
      " [1.58685904e-03 9.98413141e-01]\n",
      " [2.32172519e-07 9.99999768e-01]\n",
      " ...\n",
      " [9.98393703e-01 1.60629713e-03]\n",
      " [5.28046553e-06 9.99994720e-01]\n",
      " [9.85088650e-01 1.49113504e-02]]\n",
      "Our model predicts 98.6% correctly?\n"
     ]
    }
   ],
   "source": [
    "sequences_test,labels_test = pickle.load(open('genes_test.p','rb'))\n",
    "sequence_probabilities_0 = []\n",
    "sequence_probabilities_1 = []\n",
    "\n",
    "for s in sequences_test:\n",
    "    #! Write a function that evaluates the probability of class membership for each class by multiplying the\n",
    "    #! prior by the likelihood over all symbol transitions\n",
    "    p0 = pi_0[s[0]]\n",
    "    p1 = pi_1[s[0]]\n",
    "    for i in range(1, len(s)):\n",
    "        prev = nbd[s[i-1]]\n",
    "        new = nbd[s[i]]\n",
    "        p0 *= A_0[prev, new]\n",
    "        p1 *= A_1[prev, new]\n",
    "    norm = p0 + p1\n",
    "    p0 /= norm\n",
    "    p1 /= norm\n",
    "    sequence_probabilities_0.append(p0)\n",
    "    sequence_probabilities_1.append(p1)\n",
    "\n",
    "predictions = np.column_stack([sequence_probabilities_0, sequence_probabilities_1])\n",
    "\n",
    "# Now test our results\n",
    "successes = 0\n",
    "for i in range(len(labels_test)):\n",
    "    test_label = labels_test[i]\n",
    "    pred = predictions[i, :].argmax()\n",
    "    if pred == test_label:\n",
    "        successes += 1\n",
    "percent_correct = successes / len(labels_test) * 100\n",
    "\n",
    "print(f'Our model predicts {percent_correct}% correctly?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Turn in a document with the names of those with whom you worked, an example sequence generated by your model for each class, and a statement of your classifier's overall accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
