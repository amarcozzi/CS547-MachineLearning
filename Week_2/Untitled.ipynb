{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d460735f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25748 67183]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Maps from 'ham' or 'spam' strings to zero or one\n",
    "def mapper(s):\n",
    "    if s=='spam':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Read in the text file\n",
    "f = open('SMSSpamCollection','r')\n",
    "lines = f.readlines()\n",
    "\n",
    "# Break out the test data\n",
    "test_lines = lines[:len(lines)//5]\n",
    "lines = lines[len(lines)//5:]\n",
    "\n",
    "# Instantiate the frequency dictionary and an array to\n",
    "# record whether the line is ham or spam\n",
    "word_dictionary = {}\n",
    "training_labels = np.zeros(len(lines),dtype=int)\n",
    "\n",
    "# Loop over all the training messages\n",
    "for i,l in enumerate(lines):\n",
    "    # Split into words\n",
    "    l = l.lower().split()\n",
    "    # Record the special first word which always ham or spam\n",
    "    if l[0]=='ham':\n",
    "        training_labels[i] = 1\n",
    "    # For each word in the message, record whether the message was ham or spam\n",
    "    for w in l[1:]:\n",
    "        # If we've never seen the word before, add a new dictionary entry\n",
    "        if w not in word_dictionary:\n",
    "            word_dictionary[w] = [1,1]\n",
    "        word_dictionary[w][mapper(l[0])] += 1\n",
    "        \n",
    "# Loop over the test messages\n",
    "test_labels = np.zeros(len(test_lines),dtype=int)\n",
    "test_messages = []\n",
    "for i,l in enumerate(test_lines):\n",
    "    l = l.lower().split()\n",
    "    if l[0]=='ham':\n",
    "        test_labels[i] = 1\n",
    "    test_messages.append(l)\n",
    "\n",
    "counts = np.array([v for v in word_dictionary.values()]).sum(axis=0)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66ef375",
   "metadata": {},
   "source": [
    "Below, I have provided code skeletons. Your job is to make the code skeletons into an operational naive Bayes spam detector. (you may discard these skeletons if you would prefer to code this from scratch). Note that lines where you will need to change the code are marked with a '#!'.\n",
    "\n",
    "Your first task is train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d9782f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the prior P(Y=ham) ?\n",
    "ham_prior = counts[1] / (counts[0] + counts[1])\n",
    "spam_prior = counts[0] / (counts[0] + counts[1])\n",
    "\n",
    "# What are the class probabilities P(X=word|Y=ham) for each word?\n",
    "ham_likelihood = {}\n",
    "spam_likelihood = {}\n",
    "for key,val in word_dictionary.items():\n",
    "    ham_likelihood[key] =  val[1] / (val[0] + val[1])\n",
    "    spam_likelihood[key] = val[0] / (val[0] + val[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ca2a8",
   "metadata": {},
   "source": [
    "Your next task is to make predictions on a set of test examples which were held back from the training procedure (see *test_messages* variable).  For each of these messages, compute the ham and spam probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c12ab9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to hold the ham and spam posteriors\n",
    "posteriors = np.zeros((len(test_lines),2))\n",
    "\n",
    "# Loop over all the messages in the test set\n",
    "for i,m in enumerate(test_messages):\n",
    "    posterior_ham = ham_prior\n",
    "    posterior_spam = spam_prior\n",
    "    #! Don't forget to include the prior!\n",
    "    # Loop over all the words in each message\n",
    "    for w in m:\n",
    "        # #! What is the purpose of this try/except handler?\n",
    "        try:\n",
    "            posterior_ham *= ham_likelihood[w]\n",
    "            posterior_spam *= spam_likelihood[w]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    # Notice the normalization factor (denominator) \n",
    "    # to turn these into proper probabilities!\n",
    "    posteriors[i,0] = posterior_spam/(posterior_spam + posterior_ham)\n",
    "    posteriors[i,1] = posterior_ham/(posterior_spam + posterior_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89527f57",
   "metadata": {},
   "source": [
    "Finally, **make a ham/spam prediction based on your posterior probabilities.  Compare these to the labels contained in test_labels.  Report the accuracy of your classifier as percentage correct.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7815535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent correct = 94.0754039497307\n"
     ]
    }
   ],
   "source": [
    "sucesses = 0\n",
    "for i in range(len(test_labels)):\n",
    "    test_label = test_labels[i]\n",
    "    prediction = posteriors[i].argmax()\n",
    "    if test_label == prediction:\n",
    "        sucesses += 1\n",
    "print(f'Percent correct = {(sucesses/len(test_labels))*100}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}